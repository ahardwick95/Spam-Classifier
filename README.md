# ğŸ“© Spam vs Ham Classifier

## ğŸ“Œ Project Overview
This project explores the classic **Spam vs Ham SMS classification problem** using **Natural Language Processing (NLP)** and **Machine Learning**.  
I not only trained models on the standard dataset but also stress-tested them with **generative AIâ€“augmented datasets** (stricter imbalance + adversarial samples).  

The following is worflow for this project: **EDA â†’ Preprocessing â†’ Modeling â†’ Testing** pipeline, with additional focus on **A/B testing for class imbalance** and **generalization beyond the training corpus**.  

---

## ğŸ“‚ Dataset
- **Primary Dataset**: [SMS Spam Collection Dataset (UCI)](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)  
  - 5,574 labeled SMS messages  
  - Labels: `ham` (legit) vs `spam` (unwanted/malicious)  
  - Imbalance: ~87% ham vs ~13% spam  

- **Synthetic Datasets (Generated with LLMs)**:  
  1. **Similar Dataset** â€“ SMS-like, different from training set, with stricter class imbalance.  
  2. **Adversarial Dataset** â€“ Contains hacker/threat-actor artifacts (e.g., obfuscation, Neutral Language Injection).  

---

## ğŸ›  Workflow

### 1. EDA
- Explored spam vs ham messages **before preprocessing**.  
- Engineered new features, such as:  
  - Message length  
  - Presence of URLs
  - Digit and special-character ratios  
  - Presence of spam keywords  

---

### 2. Text Preprocessing
- Removed redundant signals from raw text (to avoid feature leakage).  
- Vectorized text using **CountVectorizer**.  
- Combined CountVectorizer output with **engineered features** â†’ created final feature matrix for modeling.  

---

### 3. Modeling (with A/B Testing)
- **Models compared**: NaÃ¯ve Bayes, Logistic Regression  
- **Experiment A**: Trained and evaluated models on **non-SMOTE** dataset  
- **Experiment B**: Trained and evaluated models on **SMOTE-augmented** dataset  
- **Findings**:  
  - Both models performed **very well without SMOTE**.  
  - SMOTE did not significantly improve performance â€” strong feature engineering likely compensated for class imbalance.  

---

### 4. Testing on New Datasets
- When tested on **synthetic similar** and **adversarial datasets**, models performed **poorly**.  
- Insight: While models fit the training data well, they **did not generalize** â€” highlighting the need for **stronger, more universal features**.  

---

## ğŸ“Š Key Results
- NaÃ¯ve Bayes and Logistic Regression achieved **high Precision, Recall and F1-scores on original dataset** (with and without SMOTE).  
- Performance **dropped sharply on synthetic and adversarial datasets**.  
- This reveals that **robust generalization** can be more important than resampling methods like SMOTE in real-world applications.  

---

## ğŸ”® Future Work
To improve feature construction and generalization, I plan to explore:  
1. **TF-IDF** â€“ better word weighting vs CountVectorizer.  
2. **N-gram & Collocation Analysis** â€“ capture context and statistically significant multi-word patterns.  
3. **Type-Token Ratio** â€“ detect repetitiveness, often present in spam.  
4. **Readability Metrics** â€“ since spam often contains hard-to-read or incomprehensible text.  

---

## ğŸš€ Tech Stack
- **Languages & Libraries**: Python, Pandas, Matplotlib, Seaborn, Regex, Scikit-learn, Imbalanced-learn, Streamlit  
- **Models**: NaÃ¯ve Bayes, Logistic Regression  
- **Deployment**: Streamlit for interactive app  

---
### Dataset Citation
- Almeida, T. & Hidalgo, J. (2011). SMS Spam Collection [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5CC84.
----
## ğŸ“· Demo
ğŸ‘‰ [Insert Screenshot or GIF of Streamlit App Here]  

---

## ğŸ“ Repository Structure
